{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab074f21-4322-49ea-bc77-9b365bcc535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import json #reading java strings into python dictionaries\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import argparse\n",
    "import re\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30bb998f-b3bc-4216-a12f-cbca0ebe5710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7840\n",
      "Data saved to 'subject_data.csv' with 7840 records.\n"
     ]
    }
   ],
   "source": [
    "#FOR ZOONIVERSE DATA\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "file_path = r\"C:\\Users\\xomad\\ICECUBE\\data\\output_retirement_lim15\\name-that-neutrino-classifications.csv\"\n",
    "subj_ids=[]\n",
    "filenames = []\n",
    "runs = []\n",
    "events = []\n",
    "ntn_subjects = pd.read_csv(file_path)\n",
    "for i in range(len(ntn_subjects)):\n",
    "    dict1 = json.loads(ntn_subjects['subject_data'][i]) #each event/subject id is its own dictionary in classifications csv. \n",
    "\n",
    "    for key, value in dict1.items():  # Iterate through each subject ID and its associated data\n",
    "        if key in subj_ids:  # Skip if subject_id is already added\n",
    "            continue\n",
    "        subj_ids.append(key)  #add the event (key) to a list\n",
    "        filename = value['Filename']\n",
    "        match = re.match(r'run_(\\d+)_event_(\\d+)_', filename)\n",
    "        \n",
    "        if match:\n",
    "            run_number = match.group(1)  # Extract the run number\n",
    "            event_number = match.group(2)  # Extract the event number\n",
    "        else:\n",
    "            run_number = None\n",
    "            event_number = None\n",
    "            print(f\"Warning: Filename {filename} does not match the expected pattern.\")\n",
    "\n",
    "         # Append extracted values to their respective lists\n",
    "        filenames.append(filename)\n",
    "        runs.append(run_number)\n",
    "        events.append(event_number)\n",
    "\n",
    "# # Combine the collected data into a DataFrame for easier handling SAVE TO NEW FILE INSTEAD\n",
    "print(len(subj_ids))\n",
    "df = pd.DataFrame({ 'subject_id': subj_ids, 'filename': filenames, 'run': runs, 'event': events})\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "df.to_csv(r'..\\data\\subject_data.csv', index=False)\n",
    "\n",
    "print(f\"Data saved to 'subject_data.csv' with {len(df)} records.\")\n",
    "\n",
    "\n",
    "\n",
    "# FOR i3 DATA\n",
    "all_sim_DNN_data = []\n",
    "events_len = 0\n",
    "\n",
    "\n",
    "MP_i3 = [f for f in os.listdir(r\"..\\MC i3 data\")]  \n",
    "\n",
    "for file in MP_i3:\n",
    "    sim_DNN_data = pd.read_excel(os.path.join(r\"..\\MC i3 data\", file))\n",
    "\n",
    "    #check how many events\n",
    "    eventsperfile = int(len(sim_DNN_data))\n",
    "    events_len += eventsperfile\n",
    "\n",
    "    all_sim_DNN_data.append(sim_DNN_data)\n",
    "\n",
    "# Concatenate all the dataframes in the list into a single dataframe\n",
    "combined_data = pd.concat(all_sim_DNN_data, ignore_index=True)\n",
    "\n",
    "# Save the combined data to a new CSV file\n",
    "combined_data.to_csv(r'..\\data\\combined_sim_DNN_data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b491365e-bc22-4cab-8f1d-145c5f81cf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched Data Columns:\n",
      "Index(['subject_id', 'filename', 'run_zoo', 'event_zoo', 'origidx',\n",
      "       'zoo_run_event_key', 'run_i3', 'event_i3', 'truth_classification',\n",
      "       'pred_skim', 'pred_cascade', 'pred_tgtrack', 'pred_starttrack',\n",
      "       'pred_stoptrack', 'energy', 'zenith', 'oneweight', 'signal_charge',\n",
      "       'bg_charge', 'qtot', 'qratio', 'log10_max_charge',\n",
      "       '#truth_classification_label', 'max_score_val', 'idx_max_score',\n",
      "       'ntn_category', 'i3_run_event_key'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#matching dnn/sim to user data, removing all duplicates \n",
    "\n",
    "file_path_zoo = r\"C:\\Users\\xomad\\ICECUBE\\data\\subject_data.csv\"\n",
    "file_path_i3 = r\"C:\\Users\\xomad\\ICECUBE\\data\\combined_sim_DNN_data.csv\"\n",
    "\n",
    "zoodata = pd.read_csv(file_path_zoo)\n",
    "i3data = pd.read_csv(file_path_i3)\n",
    "\n",
    "# Ensure 'run' and 'event' columns are treated as strings for accurate comparison\n",
    "zoodata['run'] = zoodata['run'].astype(str)\n",
    "zoodata['event'] = zoodata['event'].astype(str)\n",
    "\n",
    "i3data['run'] = i3data['run'].astype(int).astype(str)  # Remove decimals from i3_run\n",
    "i3data['event'] = i3data['event'].astype(int).astype(str)\n",
    "\n",
    "# Create unique keys for 'zoo_run_event_key' and 'i3_run_event_key'\n",
    "zoodata['zoo_run_event_key'] = zoodata['run'].astype(str) + \"_\" + zoodata['event'].astype(str)\n",
    "i3data['i3_run_event_key'] = i3data['run'].astype(str) + \"_\" + i3data['event'].astype(str)\n",
    "\n",
    "# Merge datasets on the event keys\n",
    "matched_data = pd.merge(zoodata, i3data, left_on='zoo_run_event_key', right_on='i3_run_event_key', how='inner', suffixes=('_zoo', '_i3'))\n",
    "\n",
    "# Clean the matched data by removing unwanted columns\n",
    "matched_data_cleaned = matched_data.loc[:, ~matched_data.columns.str.startswith('Unnamed')]\n",
    "\n",
    "# Find duplicate rows based on 'run_zoo' and 'event_zoo' columns and remove both duplicates\n",
    "duplicate_keys = matched_data_cleaned.duplicated(subset=['run_zoo', 'event_zoo'], keep=False) # keep = 'first' to keep 1\n",
    "matched_data_cleaned_unique = matched_data_cleaned[~duplicate_keys]\n",
    "\n",
    "# Rename columns 'run_zoo' and 'event_zoo' to 'run' and 'event'\n",
    "matched_data_cleaned_unique = matched_data_cleaned_unique.rename(columns={'run_zoo': 'run', 'event_zoo': 'event'})\n",
    "\n",
    "# Check columns\n",
    "print(\"Matched Data Columns:\")\n",
    "print(matched_data_cleaned.columns)\n",
    "\n",
    "# Save the matched data to a new CSV\n",
    "matched_data_cleaned_unique.to_csv(r'..\\data\\matched_sim_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6c5a479-8304-49c5-a43f-132117b64d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'subject_data.csv' with 7595 unique records.\n"
     ]
    }
   ],
   "source": [
    "## 1/6 used to only exclude files with (1)\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Set pandas option to display all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "file_path = r\"C:\\Users\\xomad\\ICECUBE\\data\\output_retirement_lim15\\name-that-neutrino-classifications.csv\"\n",
    "subj_ids = []\n",
    "filenames = []\n",
    "runs = []\n",
    "events = []\n",
    "origidxs = []\n",
    "\n",
    "# Read the CSV file\n",
    "ntn_subjects = pd.read_csv(file_path)\n",
    "\n",
    "# Iterate through each subject in the CSV file\n",
    "for i in range(len(ntn_subjects)):\n",
    "    # Load the subject_data (which is a JSON string in the 'subject_data' column)\n",
    "    dict1 = json.loads(ntn_subjects['subject_data'][i]) \n",
    "\n",
    "    # Iterate through each subject_id and its associated data\n",
    "    for key, value in dict1.items():\n",
    "        # Extract the filename and match it to run, event, and origidx using regex\n",
    "        filename = value['Filename']\n",
    "        \n",
    "        # Skip filenames with '(1)' in the name (duplicate versions)\n",
    "        if \"(1)\" in filename:\n",
    "            continue\n",
    "        \n",
    "        # Use regex to extract run, event, and origidx values from the filename\n",
    "        match = re.match(r'run_(\\d+)_event_(\\d+)_origidx_(\\d+)', filename)\n",
    "        \n",
    "        if match:\n",
    "            run_number = match.group(1)  # Extract the run number\n",
    "            event_number = match.group(2)  # Extract the event number\n",
    "            origidx_number = match.group(3)  # Extract the origidx number\n",
    "        else:\n",
    "            run_number = None\n",
    "            event_number = None\n",
    "            origidx_number = None\n",
    "            print(f\"Warning: Filename {filename} does not match the expected pattern.\")\n",
    "\n",
    "        # Append the subject_id, filename, run, event, and origidx to their respective lists\n",
    "        subj_ids.append(key)  # Append subject_id (key)\n",
    "        filenames.append(filename)\n",
    "        runs.append(run_number)\n",
    "        events.append(event_number)\n",
    "        origidxs.append(origidx_number)\n",
    "\n",
    "# Create a DataFrame with the collected data\n",
    "df = pd.DataFrame({\n",
    "    'subject_id': subj_ids, \n",
    "    'filename': filenames, \n",
    "    'run': runs, \n",
    "    'event': events, \n",
    "    'origidx': origidxs\n",
    "})\n",
    "\n",
    "# Remove duplicates based on the 'run', 'event', and 'origidx' columns\n",
    "df_unique = df.drop_duplicates(subset=['run', 'event', 'origidx'])\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "df_unique.to_csv(r'..\\data\\subject_data.csv', index=False)\n",
    "\n",
    "print(f\"Data saved to 'subject_data.csv' with {len(df_unique)} unique records.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04f0e7b9-b8de-4bbc-837e-3ab83149d0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining all MC i3 files into one\n",
    "all_sim_DNN_data = []\n",
    "events_len = 0\n",
    "\n",
    "\n",
    "MP_i3 = [f for f in os.listdir(r\"..\\MC i3 data\")]  \n",
    "\n",
    "for file in MP_i3:\n",
    "    sim_DNN_data = pd.read_excel(os.path.join(r\"..\\MC i3 data\", file))\n",
    "\n",
    "    #check how many events\n",
    "    eventsperfile = int(len(sim_DNN_data))\n",
    "    events_len += eventsperfile\n",
    "\n",
    "    all_sim_DNN_data.append(sim_DNN_data)\n",
    "\n",
    "# Concatenate all the dataframes in the list into a single dataframe\n",
    "combined_data = pd.concat(all_sim_DNN_data, ignore_index=True)\n",
    "\n",
    "# Save the combined data to a new CSV file\n",
    "combined_data.to_csv(r'..\\data\\combined_sim_DNN_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a1fe28-a829-436f-bac4-42520077f2b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625122c0-c6a8-4d62-a9cb-7e7a42c62b18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
